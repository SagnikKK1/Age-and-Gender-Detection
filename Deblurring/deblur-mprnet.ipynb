{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1118216,"sourceType":"datasetVersion","datasetId":627736},{"sourceId":7954906,"sourceType":"datasetVersion","datasetId":4678662},{"sourceId":7963480,"sourceType":"datasetVersion","datasetId":4684933}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pdb import set_trace as stx\n\n##########################################################################\ndef conv(in_channels, out_channels, kernel_size, bias=False, stride = 1):\n    return nn.Conv2d(\n        in_channels, out_channels, kernel_size,\n        padding=(kernel_size//2), bias=bias, stride = stride)\n\n\n##########################################################################\n## Channel Attention Layer\nclass CALayer(nn.Module):\n    def __init__(self, channel, reduction=16, bias=False):\n        super(CALayer, self).__init__()\n        # global average pooling: feature --> point\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        # feature channel downscale and upscale --> channel weight\n        self.conv_du = nn.Sequential(\n                nn.Conv2d(channel, channel // reduction, 1, padding=0, bias=bias),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel // reduction, channel, 1, padding=0, bias=bias),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.conv_du(y)\n        return x * y\n\n\n##########################################################################\n## Channel Attention Block (CAB)\nclass CAB(nn.Module):\n    def __init__(self, n_feat, kernel_size, reduction, bias, act):\n        super(CAB, self).__init__()\n        modules_body = []\n        modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n        modules_body.append(act)\n        modules_body.append(conv(n_feat, n_feat, kernel_size, bias=bias))\n\n        self.CA = CALayer(n_feat, reduction, bias=bias)\n        self.body = nn.Sequential(*modules_body)\n\n    def forward(self, x):\n        res = self.body(x)\n        res = self.CA(res)\n        res += x\n        return res\n\n##########################################################################\n## Supervised Attention Module\nclass SAM(nn.Module):\n    def __init__(self, n_feat, kernel_size, bias):\n        super(SAM, self).__init__()\n        self.conv1 = conv(n_feat, n_feat, kernel_size, bias=bias)\n        self.conv2 = conv(n_feat, 3, kernel_size, bias=bias)\n        self.conv3 = conv(3, n_feat, kernel_size, bias=bias)\n\n    def forward(self, x, x_img):\n        x1 = self.conv1(x)\n        img = self.conv2(x) + x_img\n        x2 = torch.sigmoid(self.conv3(img))\n        x1 = x1*x2\n        x1 = x1+x\n        return x1, img\n\n##########################################################################\n## U-Net\n\nclass Encoder(nn.Module):\n    def __init__(self, n_feat, kernel_size, reduction, act, bias, scale_unetfeats, csff):\n        super(Encoder, self).__init__()\n\n        self.encoder_level1 = [CAB(n_feat,                     kernel_size, reduction, bias=bias, act=act) for _ in range(2)]\n        self.encoder_level2 = [CAB(n_feat+scale_unetfeats,     kernel_size, reduction, bias=bias, act=act) for _ in range(2)]\n        self.encoder_level3 = [CAB(n_feat+(scale_unetfeats*2), kernel_size, reduction, bias=bias, act=act) for _ in range(2)]\n\n        self.encoder_level1 = nn.Sequential(*self.encoder_level1)\n        self.encoder_level2 = nn.Sequential(*self.encoder_level2)\n        self.encoder_level3 = nn.Sequential(*self.encoder_level3)\n\n        self.down12  = DownSample(n_feat, scale_unetfeats)\n        self.down23  = DownSample(n_feat+scale_unetfeats, scale_unetfeats)\n\n        # Cross Stage Feature Fusion (CSFF)\n        if csff:\n            self.csff_enc1 = nn.Conv2d(n_feat,                     n_feat,                     kernel_size=1, bias=bias)\n            self.csff_enc2 = nn.Conv2d(n_feat+scale_unetfeats,     n_feat+scale_unetfeats,     kernel_size=1, bias=bias)\n            self.csff_enc3 = nn.Conv2d(n_feat+(scale_unetfeats*2), n_feat+(scale_unetfeats*2), kernel_size=1, bias=bias)\n\n            self.csff_dec1 = nn.Conv2d(n_feat,                     n_feat,                     kernel_size=1, bias=bias)\n            self.csff_dec2 = nn.Conv2d(n_feat+scale_unetfeats,     n_feat+scale_unetfeats,     kernel_size=1, bias=bias)\n            self.csff_dec3 = nn.Conv2d(n_feat+(scale_unetfeats*2), n_feat+(scale_unetfeats*2), kernel_size=1, bias=bias)\n\n    def forward(self, x, encoder_outs=None, decoder_outs=None):\n        enc1 = self.encoder_level1(x)\n        if (encoder_outs is not None) and (decoder_outs is not None):\n            enc1 = enc1 + self.csff_enc1(encoder_outs[0]) + self.csff_dec1(decoder_outs[0])\n\n        x = self.down12(enc1)\n\n        enc2 = self.encoder_level2(x)\n        if (encoder_outs is not None) and (decoder_outs is not None):\n            enc2 = enc2 + self.csff_enc2(encoder_outs[1]) + self.csff_dec2(decoder_outs[1])\n\n        x = self.down23(enc2)\n\n        enc3 = self.encoder_level3(x)\n        if (encoder_outs is not None) and (decoder_outs is not None):\n            enc3 = enc3 + self.csff_enc3(encoder_outs[2]) + self.csff_dec3(decoder_outs[2])\n        \n        return [enc1, enc2, enc3]\n\nclass Decoder(nn.Module):\n    def __init__(self, n_feat, kernel_size, reduction, act, bias, scale_unetfeats):\n        super(Decoder, self).__init__()\n\n        self.decoder_level1 = [CAB(n_feat,                     kernel_size, reduction, bias=bias, act=act) for _ in range(2)]\n        self.decoder_level2 = [CAB(n_feat+scale_unetfeats,     kernel_size, reduction, bias=bias, act=act) for _ in range(2)]\n        self.decoder_level3 = [CAB(n_feat+(scale_unetfeats*2), kernel_size, reduction, bias=bias, act=act) for _ in range(2)]\n\n        self.decoder_level1 = nn.Sequential(*self.decoder_level1)\n        self.decoder_level2 = nn.Sequential(*self.decoder_level2)\n        self.decoder_level3 = nn.Sequential(*self.decoder_level3)\n\n        self.skip_attn1 = CAB(n_feat,                 kernel_size, reduction, bias=bias, act=act)\n        self.skip_attn2 = CAB(n_feat+scale_unetfeats, kernel_size, reduction, bias=bias, act=act)\n\n        self.up21  = SkipUpSample(n_feat, scale_unetfeats)\n        self.up32  = SkipUpSample(n_feat+scale_unetfeats, scale_unetfeats)\n\n    def forward(self, outs):\n        enc1, enc2, enc3 = outs\n        dec3 = self.decoder_level3(enc3)\n\n        x = self.up32(dec3, self.skip_attn2(enc2))\n        dec2 = self.decoder_level2(x)\n\n        x = self.up21(dec2, self.skip_attn1(enc1))\n        dec1 = self.decoder_level1(x)\n\n        return [dec1,dec2,dec3]\n\n##########################################################################\n##---------- Resizing Modules ----------    \nclass DownSample(nn.Module):\n    def __init__(self, in_channels,s_factor):\n        super(DownSample, self).__init__()\n        self.down = nn.Sequential(nn.Upsample(scale_factor=0.5, mode='bilinear', align_corners=False),\n                                  nn.Conv2d(in_channels, in_channels+s_factor, 1, stride=1, padding=0, bias=False))\n\n    def forward(self, x):\n        x = self.down(x)\n        return x\n\nclass UpSample(nn.Module):\n    def __init__(self, in_channels,s_factor):\n        super(UpSample, self).__init__()\n        self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n                                nn.Conv2d(in_channels+s_factor, in_channels, 1, stride=1, padding=0, bias=False))\n\n    def forward(self, x):\n        x = self.up(x)\n        return x\n\nclass SkipUpSample(nn.Module):\n    def __init__(self, in_channels,s_factor):\n        super(SkipUpSample, self).__init__()\n        self.up = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n                                nn.Conv2d(in_channels+s_factor, in_channels, 1, stride=1, padding=0, bias=False))\n\n    def forward(self, x, y):\n        x = self.up(x)\n        x = x + y\n        return x\n\n##########################################################################\n## Original Resolution Block (ORB)\nclass ORB(nn.Module):\n    def __init__(self, n_feat, kernel_size, reduction, act, bias, num_cab):\n        super(ORB, self).__init__()\n        modules_body = []\n        modules_body = [CAB(n_feat, kernel_size, reduction, bias=bias, act=act) for _ in range(num_cab)]\n        modules_body.append(conv(n_feat, n_feat, kernel_size))\n        self.body = nn.Sequential(*modules_body)\n\n    def forward(self, x):\n        res = self.body(x)\n        res += x\n        return res\n\n##########################################################################\nclass ORSNet(nn.Module):\n    def __init__(self, n_feat, scale_orsnetfeats, kernel_size, reduction, act, bias, scale_unetfeats, num_cab):\n        super(ORSNet, self).__init__()\n\n        self.orb1 = ORB(n_feat+scale_orsnetfeats, kernel_size, reduction, act, bias, num_cab)\n        self.orb2 = ORB(n_feat+scale_orsnetfeats, kernel_size, reduction, act, bias, num_cab)\n        self.orb3 = ORB(n_feat+scale_orsnetfeats, kernel_size, reduction, act, bias, num_cab)\n\n        self.up_enc1 = UpSample(n_feat, scale_unetfeats)\n        self.up_dec1 = UpSample(n_feat, scale_unetfeats)\n\n        self.up_enc2 = nn.Sequential(UpSample(n_feat+scale_unetfeats, scale_unetfeats), UpSample(n_feat, scale_unetfeats))\n        self.up_dec2 = nn.Sequential(UpSample(n_feat+scale_unetfeats, scale_unetfeats), UpSample(n_feat, scale_unetfeats))\n\n        self.conv_enc1 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, kernel_size=1, bias=bias)\n        self.conv_enc2 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, kernel_size=1, bias=bias)\n        self.conv_enc3 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, kernel_size=1, bias=bias)\n\n        self.conv_dec1 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, kernel_size=1, bias=bias)\n        self.conv_dec2 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, kernel_size=1, bias=bias)\n        self.conv_dec3 = nn.Conv2d(n_feat, n_feat+scale_orsnetfeats, kernel_size=1, bias=bias)\n\n    def forward(self, x, encoder_outs, decoder_outs):\n        x = self.orb1(x)\n        x = x + self.conv_enc1(encoder_outs[0]) + self.conv_dec1(decoder_outs[0])\n\n        x = self.orb2(x)\n        x = x + self.conv_enc2(self.up_enc1(encoder_outs[1])) + self.conv_dec2(self.up_dec1(decoder_outs[1]))\n\n        x = self.orb3(x)\n        x = x + self.conv_enc3(self.up_enc2(encoder_outs[2])) + self.conv_dec3(self.up_dec2(decoder_outs[2]))\n\n        return x\n\n\n##########################################################################\nclass MPRNet(nn.Module):\n    def __init__(self, in_c=3, out_c=3, n_feat=96, scale_unetfeats=48, scale_orsnetfeats=32, num_cab=8, kernel_size=3, reduction=4, bias=False):\n        super(MPRNet, self).__init__()\n\n        act=nn.PReLU()\n        self.shallow_feat1 = nn.Sequential(conv(in_c, n_feat, kernel_size, bias=bias), CAB(n_feat,kernel_size, reduction, bias=bias, act=act))\n        self.shallow_feat2 = nn.Sequential(conv(in_c, n_feat, kernel_size, bias=bias), CAB(n_feat,kernel_size, reduction, bias=bias, act=act))\n        self.shallow_feat3 = nn.Sequential(conv(in_c, n_feat, kernel_size, bias=bias), CAB(n_feat,kernel_size, reduction, bias=bias, act=act))\n\n        # Cross Stage Feature Fusion (CSFF)\n        self.stage1_encoder = Encoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats, csff=False)\n        self.stage1_decoder = Decoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats)\n\n        self.stage2_encoder = Encoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats, csff=True)\n        self.stage2_decoder = Decoder(n_feat, kernel_size, reduction, act, bias, scale_unetfeats)\n\n        self.stage3_orsnet = ORSNet(n_feat, scale_orsnetfeats, kernel_size, reduction, act, bias, scale_unetfeats, num_cab)\n\n        self.sam12 = SAM(n_feat, kernel_size=1, bias=bias)\n        self.sam23 = SAM(n_feat, kernel_size=1, bias=bias)\n        \n        self.concat12  = conv(n_feat*2, n_feat, kernel_size, bias=bias)\n        self.concat23  = conv(n_feat*2, n_feat+scale_orsnetfeats, kernel_size, bias=bias)\n        self.tail     = conv(n_feat+scale_orsnetfeats, out_c, kernel_size, bias=bias)\n\n    def forward(self, x3_img):\n        # Original-resolution Image for Stage 3\n        H = x3_img.size(2)\n        W = x3_img.size(3)\n\n        # Multi-Patch Hierarchy: Split Image into four non-overlapping patches\n\n        # Two Patches for Stage 2\n        x2top_img  = x3_img[:,:,0:int(H/2),:]\n        x2bot_img  = x3_img[:,:,int(H/2):H,:]\n\n        # Four Patches for Stage 1\n        x1ltop_img = x2top_img[:,:,:,0:int(W/2)]\n        x1rtop_img = x2top_img[:,:,:,int(W/2):W]\n        x1lbot_img = x2bot_img[:,:,:,0:int(W/2)]\n        x1rbot_img = x2bot_img[:,:,:,int(W/2):W]\n\n        ##-------------------------------------------\n        ##-------------- Stage 1---------------------\n        ##-------------------------------------------\n        ## Compute Shallow Features\n        x1ltop = self.shallow_feat1(x1ltop_img)\n        x1rtop = self.shallow_feat1(x1rtop_img)\n        x1lbot = self.shallow_feat1(x1lbot_img)\n        x1rbot = self.shallow_feat1(x1rbot_img)\n        \n        ## Process features of all 4 patches with Encoder of Stage 1\n        feat1_ltop = self.stage1_encoder(x1ltop)\n        feat1_rtop = self.stage1_encoder(x1rtop)\n        feat1_lbot = self.stage1_encoder(x1lbot)\n        feat1_rbot = self.stage1_encoder(x1rbot)\n        \n        ## Concat deep features\n        feat1_top = [torch.cat((k,v), 3) for k,v in zip(feat1_ltop,feat1_rtop)]\n        feat1_bot = [torch.cat((k,v), 3) for k,v in zip(feat1_lbot,feat1_rbot)]\n        \n        ## Pass features through Decoder of Stage 1\n        res1_top = self.stage1_decoder(feat1_top)\n        res1_bot = self.stage1_decoder(feat1_bot)\n\n        ## Apply Supervised Attention Module (SAM)\n        x2top_samfeats, stage1_img_top = self.sam12(res1_top[0], x2top_img)\n        x2bot_samfeats, stage1_img_bot = self.sam12(res1_bot[0], x2bot_img)\n\n        ## Output image at Stage 1\n        stage1_img = torch.cat([stage1_img_top, stage1_img_bot],2) \n        ##-------------------------------------------\n        ##-------------- Stage 2---------------------\n        ##-------------------------------------------\n        ## Compute Shallow Features\n        x2top  = self.shallow_feat2(x2top_img)\n        x2bot  = self.shallow_feat2(x2bot_img)\n\n        ## Concatenate SAM features of Stage 1 with shallow features of Stage 2\n        x2top_cat = self.concat12(torch.cat([x2top, x2top_samfeats], 1))\n        x2bot_cat = self.concat12(torch.cat([x2bot, x2bot_samfeats], 1))\n\n        ## Process features of both patches with Encoder of Stage 2\n        feat2_top = self.stage2_encoder(x2top_cat, feat1_top, res1_top)\n        feat2_bot = self.stage2_encoder(x2bot_cat, feat1_bot, res1_bot)\n\n        ## Concat deep features\n        feat2 = [torch.cat((k,v), 2) for k,v in zip(feat2_top,feat2_bot)]\n\n        ## Pass features through Decoder of Stage 2\n        res2 = self.stage2_decoder(feat2)\n\n        ## Apply SAM\n        x3_samfeats, stage2_img = self.sam23(res2[0], x3_img)\n\n\n        ##-------------------------------------------\n        ##-------------- Stage 3---------------------\n        ##-------------------------------------------\n        ## Compute Shallow Features\n        x3     = self.shallow_feat3(x3_img)\n\n        ## Concatenate SAM features of Stage 2 with shallow features of Stage 3\n        x3_cat = self.concat23(torch.cat([x3, x3_samfeats], 1))\n        \n        x3_cat = self.stage3_orsnet(x3_cat, feat2, res2)\n\n        stage3_img = self.tail(x3_cat)\n\n        return [stage3_img+x3_img, stage2_img, stage1_img]","metadata":{"execution":{"iopub.status.busy":"2024-03-28T10:35:14.784339Z","iopub.execute_input":"2024-03-28T10:35:14.784748Z","iopub.status.idle":"2024-03-28T10:35:18.706835Z","shell.execute_reply.started":"2024-03-28T10:35:14.784712Z","shell.execute_reply":"2024-03-28T10:35:18.705792Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms\nfrom PIL import Image\nfrom torch.utils.data import DataLoader, Dataset\nimport os\n\n# Define your device (GPU if available, else CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define your MPRNet model and move it to the device\nmodel = model.to(device)\n\n# Load pretrained weights\nstate_dict_path = \"/kaggle/input/checkpoint/model_deblurring.pth\"\nloaded_state_dict = torch.load(state_dict_path, map_location=device)\n\nif \"state_dict\" in loaded_state_dict:\n    state_dict = loaded_state_dict[\"state_dict\"]\nelse:\n    state_dict = loaded_state_dict\nmodel.load_state_dict(state_dict)\n\n# Define loss function (e.g., Mean Squared Error loss)\ncriterion = nn.MSELoss().to(device)\n\n# Define optimizer (e.g., Adam optimizer)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Define transformations for data augmentation (if needed)\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # Convert PIL image to tensor\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize image pixels\n])\n\n# Define dataset class\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\n\n# class DeblurDataset(Dataset):\n#     def __init__(self, blur_folder, sharp_folder, transform=None):\n#         self.blur_folder = blur_folder\n#         self.sharp_folder = sharp_folder\n#         self.blur_images = os.listdir(blur_folder)\n#         self.sharp_images = os.listdir(sharp_folder)\n#         self.transform = transform\n\n#     def __len__(self):\n#         return min(len(self.blur_images), len(self.sharp_images))\n\n#     def __getitem__(self, idx):\n#         blur_img_name = os.path.join(self.blur_folder, self.blur_images[idx])\n#         sharp_img_name = os.path.join(self.sharp_folder, self.sharp_images[idx])\n#         blur_image = Image.open(blur_img_name).convert(\"RGB\")\n#         sharp_image = Image.open(sharp_img_name).convert(\"RGB\")\n\n#         if self.transform:\n#             blur_image = self.transform(blur_image)\n#             sharp_image = self.transform(sharp_image)\n\n#         return blur_image, sharp_image\n\n\nclass DeblurDataset(Dataset):\n    def __init__(self, blur_folder, sharp_folder, transform=None):\n        self.blur_folder = blur_folder\n        self.sharp_folder = sharp_folder\n        self.blur_images = os.listdir(blur_folder)\n        self.sharp_images = os.listdir(sharp_folder)\n        self.transform = transform\n\n    def __len__(self):\n        return min(len(self.blur_images), len(self.sharp_images))\n\n    def __getitem__(self, idx):\n        blur_img_name = os.path.join(self.blur_folder, self.blur_images[idx])\n        sharp_img_name = os.path.join(self.sharp_folder, self.sharp_images[idx])\n        with Image.open(blur_img_name).convert(\"RGB\") as blur_image, Image.open(sharp_img_name).convert(\"RGB\") as sharp_image:\n            if self.transform:\n                blur_image = self.transform(blur_image)\n                sharp_image = self.transform(sharp_image)\n        return blur_image.to(device), sharp_image.to(device)\n\n\n\n# Define transformation to apply on the images\ntransform = transforms.Compose([\n    transforms.ToTensor(),  # Convert PIL image to tensor\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize image pixels\n])\n\n# Define paths to the blur and sharp image folders\nblur_folder = \"/kaggle/input/gopro-deblur/gopro_deblur/blur/images\"\nsharp_folder = \"/kaggle/input/gopro-deblur/gopro_deblur/sharp/images\"\n\n# Create dataset\ndataset = DeblurDataset(blur_folder, sharp_folder, transform=transform)\n\n# Create data loader\nbatch_size = 4\ntrain_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Now you can use train_loader in your training loop\n\n\n# Set model to training mode\nmodel.train()\n\n# Training loop\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n        inputs, labels = data\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs, _, _ = model(inputs)\n\n        # Compute loss\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n\n        # Print statistics\n        running_loss += loss.item()\n        if i % 10 == 9:  # Print every 10 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss / 10))\n            running_loss = 0.0\n\nprint('Finished Training')\n","metadata":{"execution":{"iopub.status.busy":"2024-03-28T10:41:07.879245Z","iopub.execute_input":"2024-03-28T10:41:07.880011Z","iopub.status.idle":"2024-03-28T10:41:09.204949Z","shell.execute_reply.started":"2024-03-28T10:41:07.879979Z","shell.execute_reply":"2024-03-28T10:41:09.203579Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 117\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    116\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    118\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    119\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 44.12 MiB is free. Process 3034 has 15.85 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 132.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 44.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 44.12 MiB is free. Process 3034 has 15.85 GiB memory in use. Of the allocated memory 15.43 GiB is allocated by PyTorch, and 132.25 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}